{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "718a467a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import snscrape.modules.twitter as sntwitter  # only works with python 3.11 or lower\n",
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import spacy\n",
    "import emoji\n",
    "from textblob import TextBlob\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "from decouple import config\n",
    "import tweepy\n",
    "from datetime import datetime, timedelta, timezone\n",
    "import urllib\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38d4e3b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Authentication successful!\n"
     ]
    }
   ],
   "source": [
    "# Load environment variables and Authenticate via tweepy\n",
    "\n",
    "BEARER_TOKEN = config(\"BEARER_TOKEN\")\n",
    "\n",
    "# Create a Tweepy Client instance for API v2\n",
    "# This is the modern and recommended way to use Tweepy\n",
    "try:\n",
    "    client = tweepy.Client(BEARER_TOKEN)\n",
    "    print(\"Authentication successful!\")\n",
    "except Exception as e:\n",
    "    print(f\"Error during authentication: {e}\")\n",
    "    # If authentication fails, we stop here.\n",
    "    # Make sure your Bearer Token is correct.\n",
    "    client = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8df50ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_query = \"#ElectricVehicles lang:en -is:retweet\"\n",
    "query = urllib.parse.quote(raw_query)\n",
    "\n",
    "max_results = 10  # Max allowed in one request under Free tier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "24df14ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tweets metadata\n",
    "tweet_fields = \"tweet.fields=created_at,author_id,lang,public_metrics,conversation_id\"\n",
    "user_fields = \"user.fields=username,name,created_at,public_metrics,verified\"\n",
    "expansions = \"expansions=author_id\"\n",
    "\n",
    "url = (\n",
    "    f\"https://api.twitter.com/2/tweets/search/recent\"\n",
    "    f\"?query={query}&max_results={max_results}&{tweet_fields}&{user_fields}&{expansions}\"\n",
    ")\n",
    "\n",
    "headers = {\n",
    "    \"Authorization\": f\"Bearer {BEARER_TOKEN}\",\n",
    "    \"User-Agent\": \"v2RecentSearchPython\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "565bff88",
   "metadata": {},
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "Request returned an error: 429 {\"title\":\"Too Many Requests\",\"detail\":\"Too Many Requests\",\"type\":\"about:blank\",\"status\":429}",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mException\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m response = requests.get(url, headers=headers)\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m response.status_code != \u001b[32m200\u001b[39m:\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mRequest returned an error: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse.status_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse.text\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m      5\u001b[39m     data = response.json()\n",
      "\u001b[31mException\u001b[39m: Request returned an error: 429 {\"title\":\"Too Many Requests\",\"detail\":\"Too Many Requests\",\"type\":\"about:blank\",\"status\":429}"
     ]
    }
   ],
   "source": [
    "response = requests.get(url, headers=headers)\n",
    "if response.status_code != 200:\n",
    "    raise Exception(f\"Request returned an error: {response.status_code} {response.text}\")\n",
    "else:\n",
    "    data = response.json()\n",
    "\n",
    "users = {u[\"id\"]: u for u in data.get(\"includes\", {}).get(\"users\", [])}\n",
    "\n",
    "results = []\n",
    "for tweet in data.get(\"data\", []):\n",
    "    user = users.get(tweet[\"author_id\"], {})\n",
    "    tweet_metrics = tweet.get(\"public_metrics\", {})\n",
    "    user_metrics = user.get(\"public_metrics\", {})\n",
    "\n",
    "    results.append({\n",
    "        \"tweet_id\": tweet[\"id\"],\n",
    "        \"text\": tweet[\"text\"],\n",
    "        \"created_at\": tweet[\"created_at\"],\n",
    "        \"language\": tweet[\"lang\"],\n",
    "        \"conversation_id\": tweet.get(\"conversation_id\"),\n",
    "        \"retweets\": tweet_metrics.get(\"retweet_count\"),\n",
    "        \"likes\": tweet_metrics.get(\"like_count\"),\n",
    "        \"replies\": tweet_metrics.get(\"reply_count\"),\n",
    "        \"quotes\": tweet_metrics.get(\"quote_count\"),\n",
    "        \"author_id\": tweet[\"author_id\"],\n",
    "        \"username\": user.get(\"username\"),\n",
    "        \"name\": user.get(\"name\"),\n",
    "        \"user_created_at\": user.get(\"created_at\"),\n",
    "        \"verified\": user.get(\"verified\"),\n",
    "        \"followers\": user_metrics.get(\"followers_count\"),\n",
    "        \"following\": user_metrics.get(\"following_count\"),\n",
    "        \"tweet_count\": user_metrics.get(\"tweet_count\")\n",
    "    })\n",
    "\n",
    "df = pd.DataFrame(results)\n",
    "df.to_csv(\"ev_tweets.csv\", index=False)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "543e51ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Successfully fetched 10 tweets.\n",
      "\n",
      "--- Fetched Tweets ---\n",
      "                                               tweet\n",
      "0  Market Share of Top Electric 2-Wheeler Compani...\n",
      "1  CATL has started buying lithium ore from exter...\n",
      "2  “#ElectricVehicles bring health benefits &amp;...\n",
      "3  “#ElectricVehicles bring health benefits &amp;...\n",
      "4  “#ElectricVehicles bring health benefits and c...\n",
      "\n",
      "Tweets saved to ev_tweets.csv\n"
     ]
    }
   ],
   "source": [
    "# --- SEARCHING FOR TWEETS ---\n",
    "\n",
    "if client:\n",
    "    # 1. Define search query.\n",
    "    # You can use keywords, hashtags, or more complex rules.\n",
    "    # Search for tweets about #EVs.\n",
    "    # Add '-is:retweet' to exclude retweets and get more original content.\n",
    "    # We also specify 'lang:en' to get English-language tweets.\n",
    "    query = \"#ElectricVehicles -is:retweet lang:en\"\n",
    "\n",
    "    # 2. Use the search_recent_tweets method.\n",
    "    # - The 'query' parameter is what we're searching for.\n",
    "    # - The 'max_results' parameter specifies how many tweets to return (10 to 100).\n",
    "    #   Let's fetch 10.\n",
    "    try:\n",
    "        response = client.search_recent_tweets(query=query, max_results=10)\n",
    "\n",
    "        # The response object contains the data and other metadata.\n",
    "        # The actual tweets are in response.data.\n",
    "        tweets = response.data\n",
    "\n",
    "        # 3. Check if any tweets were found.\n",
    "        if tweets:\n",
    "            print(f\"\\nSuccessfully fetched {len(tweets)} tweets.\")\n",
    "\n",
    "            # 4. Create a list to hold the tweet text.\n",
    "            tweet_texts = []\n",
    "            for tweet in tweets:\n",
    "                tweet_texts.append(tweet.text)\n",
    "\n",
    "            # 5. Convert the list into a pandas DataFrame.\n",
    "            # This is the format you'll need for the next steps of your assignment.\n",
    "            df = pd.DataFrame(tweet_texts, columns=[\"tweet\"])\n",
    "\n",
    "            # Display the first few tweets\n",
    "            print(\"\\n--- Fetched Tweets ---\")\n",
    "            print(df.head())\n",
    "\n",
    "            # 6. Save the tweets to a CSV file for later use.\n",
    "            # This is a good practice so you don't have to fetch them every time.\n",
    "            df.to_csv(\"ev_tweets.csv\", index=False)\n",
    "            print(\"\\nTweets saved to ev_tweets.csv\")\n",
    "\n",
    "        else:\n",
    "            print(\"No tweets found for your query. Try a different keyword or hashtag.\")\n",
    "    \n",
    "    except tweepy.errors.TweepyException as e:\n",
    "        # Handles API-specific errors, like the 429 Too Many Requests\n",
    "        print(f\"A Tweepy error occurred: {e}\")\n",
    "    except ConnectionError as e:\n",
    "        # Specifically catches network-related errors like the one you saw\n",
    "        print(f\"A network connection error occurred: {e}\")\n",
    "        print(\"This is often temporary. Please check your internet connection and try again in a few moments.\")\n",
    "    except Exception as e:\n",
    "        # Catches any other unexpected errors\n",
    "        print(f\"An unexpected error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6d55b8d",
   "metadata": {},
   "source": [
    "## 2. Data Cleaning and Preparation\n",
    "\n",
    "Clean and prepare the tweet text data for sentiment analysis.\n",
    "The steps include:\n",
    "\n",
    "1. Removing URLs, mentions, hashtags (or removing the ‘#’ but keeping the word), emojis and special characters\n",
    "2. Converting text to lowercase\n",
    "3. Tokenizing the text\n",
    "4. Removing stop-words\n",
    "5. Lemmatizing the tokens\n",
    "6. Optionally rebuilding the cleaned text field\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4a071ae",
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[E050] Can't find model 'en_core_web_sm'. It doesn't seem to be a Python package or a valid path to a data directory.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOSError\u001b[39m                                   Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Load spaCy English model\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m nlp = \u001b[43mspacy\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43men_core_web_sm\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# Define stopwords\u001b[39;00m\n\u001b[32m      5\u001b[39m stop_words = \u001b[38;5;28mset\u001b[39m(stopwords.words(\u001b[33m'\u001b[39m\u001b[33menglish\u001b[39m\u001b[33m'\u001b[39m))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.virtualenvs/Applications-of-AI-ItjPKJgw/lib/python3.11/site-packages/spacy/__init__.py:52\u001b[39m, in \u001b[36mload\u001b[39m\u001b[34m(name, vocab, disable, enable, exclude, config)\u001b[39m\n\u001b[32m     28\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mload\u001b[39m(\n\u001b[32m     29\u001b[39m     name: Union[\u001b[38;5;28mstr\u001b[39m, Path],\n\u001b[32m     30\u001b[39m     *,\n\u001b[32m   (...)\u001b[39m\u001b[32m     35\u001b[39m     config: Union[Dict[\u001b[38;5;28mstr\u001b[39m, Any], Config] = util.SimpleFrozenDict(),\n\u001b[32m     36\u001b[39m ) -> Language:\n\u001b[32m     37\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Load a spaCy model from an installed package or a local path.\u001b[39;00m\n\u001b[32m     38\u001b[39m \n\u001b[32m     39\u001b[39m \u001b[33;03m    name (str): Package name or model path.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     50\u001b[39m \u001b[33;03m    RETURNS (Language): The loaded nlp object.\u001b[39;00m\n\u001b[32m     51\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mutil\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     53\u001b[39m \u001b[43m        \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     54\u001b[39m \u001b[43m        \u001b[49m\u001b[43mvocab\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvocab\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     55\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdisable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdisable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     56\u001b[39m \u001b[43m        \u001b[49m\u001b[43menable\u001b[49m\u001b[43m=\u001b[49m\u001b[43menable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     57\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexclude\u001b[49m\u001b[43m=\u001b[49m\u001b[43mexclude\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     58\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     59\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.virtualenvs/Applications-of-AI-ItjPKJgw/lib/python3.11/site-packages/spacy/util.py:484\u001b[39m, in \u001b[36mload_model\u001b[39m\u001b[34m(name, vocab, disable, enable, exclude, config)\u001b[39m\n\u001b[32m    482\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m OLD_MODEL_SHORTCUTS:\n\u001b[32m    483\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIOError\u001b[39;00m(Errors.E941.format(name=name, full=OLD_MODEL_SHORTCUTS[name]))  \u001b[38;5;66;03m# type: ignore[index]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m484\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIOError\u001b[39;00m(Errors.E050.format(name=name))\n",
      "\u001b[31mOSError\u001b[39m: [E050] Can't find model 'en_core_web_sm'. It doesn't seem to be a Python package or a valid path to a data directory."
     ]
    }
   ],
   "source": [
    "# Load spaCy English model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Define stopwords\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "\n",
    "# A function to remove URLs, mentions, hashtags (#), emojis, special chars\n",
    "def clean_tweet_text(text):\n",
    "    # Remove URLs\n",
    "    text = re.sub(r\"http\\S+|www.\\S+\", \"\", text)\n",
    "    # Remove mentions (@username)\n",
    "    text = re.sub(r\"@\\w+\", \"\", text)\n",
    "    # Remove hashtag symbol (keep the word) – e.g. \"#ElectricVehicles\" → \"ElectricVehicles\"\n",
    "    text = re.sub(r\"#\", \"\", text)\n",
    "    # Demojize (optional) or simply remove emojis\n",
    "    text = emoji.replace_emoji(text, replace=\"\")\n",
    "    # Remove punctuation and special characters\n",
    "    text = re.sub(f\"[{re.escape(string.punctuation)}]\", \"\", text)\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    return text\n",
    "\n",
    "\n",
    "# A function to tokenize, remove stopwords, and lemmatize\n",
    "def tokenize_lemmatize(text):\n",
    "    # Tokenize\n",
    "    tokens = word_tokenize(text)\n",
    "    # Remove stopwords and tokens of length <= 1\n",
    "    tokens = [tok for tok in tokens if tok not in stop_words and len(tok) > 1]\n",
    "    # Lemmatize using spaCy\n",
    "    doc = nlp(\" \".join(tokens))\n",
    "    lemmas = [token.lemma_ for token in doc]\n",
    "    return lemmas\n",
    "\n",
    "\n",
    "# Example usage: load the data\n",
    "df = pd.read_csv(\"tweets_evs.csv\")  # make sure your filename matches\n",
    "# Apply cleaning\n",
    "df[\"clean_text\"] = df[\"Tweet\"].apply(clean_tweet_text)\n",
    "df[\"tokens\"] = df[\"clean_text\"].apply(tokenize_lemmatize)\n",
    "# Optionally join tokens back to a cleaned string\n",
    "df[\"cleaned_joined\"] = df[\"tokens\"].apply(lambda x: \" \".join(x))\n",
    "\n",
    "# Show the first few cleaned rows\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e58609d4",
   "metadata": {},
   "source": [
    "### Explanation of cleaning steps:\n",
    "\n",
    "- We start by removing URLs and mentions because they don’t contribute to sentiment in most cases and may bias the text.\n",
    "- We remove the “#” symbol but keep the keyword after it so that hashtags still contribute their meaning (for example, ‘ElectricVehicles’ becomes a regular word).\n",
    "- Emojis can carry sentiment but for simplicity we remove them here (you could instead convert emojis to text meaning using `emoji.demojize`).\n",
    "- Punctuation and special characters are removed to reduce noise.\n",
    "- Lowercasing ensures that words like “EVs”, “evs”, and “Evs” are treated the same.\n",
    "- Tokenization breaks the text into individual tokens/words.\n",
    "- Stop-words removal strips out very common words (“the”, “is”, “and”) that don’t carry sentiment by themselves.\n",
    "- Lemmatization reduces words to their base form (“running” → “run”, “better” → “good”), which helps with grouping similar concepts.\n",
    "- Finally, we rebuild a cleaned version of the text for downstream analysis (sentiment scoring, word-cloud, etc).\n",
    "\n",
    "```python\n",
    "# Save cleaned data (optional)\n",
    "df.to_csv(\"tweets_evs_cleaned.csv\", index=False)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa003e1a",
   "metadata": {},
   "source": [
    "## 3. Sentiment Analysis\n",
    "\n",
    "In this section, we will analyze the sentiment of our cleaned tweets using three different approaches:\n",
    "\n",
    "1. **TextBlob** – simple polarity-based sentiment scoring\n",
    "2. **VADER (Valence Aware Dictionary for sEntiment Reasoning)** – optimized for social media text\n",
    "3. _(Optional)_ **Transformer model (HuggingFace)** – a pretrained deep learning model for more accurate sentiment classification\n",
    "\n",
    "For each tweet, we will compute its sentiment polarity and then visualize the distribution of sentiments (positive, neutral, negative).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "557cc161",
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Load cleaned data\n",
    "df = pd.read_csv(\"tweets_evs_cleaned.csv\")\n",
    "\n",
    "# Compute polarity: range is [-1.0, 1.0]\n",
    "df[\"polarity\"] = df[\"cleaned_joined\"].apply(\n",
    "    lambda x: TextBlob(str(x)).sentiment.polarity\n",
    ")\n",
    "\n",
    "\n",
    "# Define sentiment categories\n",
    "def get_sentiment_label(p):\n",
    "    if p > 0.05:\n",
    "        return \"Positive\"\n",
    "    elif p < -0.05:\n",
    "        return \"Negative\"\n",
    "    else:\n",
    "        return \"Neutral\"\n",
    "\n",
    "\n",
    "df[\"sentiment\"] = df[\"polarity\"].apply(get_sentiment_label)\n",
    "\n",
    "# Display sample results\n",
    "df[[\"cleaned_joined\", \"polarity\", \"sentiment\"]].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f07469af",
   "metadata": {},
   "source": [
    "### Visualizing sentiment distribution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45ba8c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentiment distribution\n",
    "sns.set(style=\"whitegrid\")\n",
    "plt.figure(figsize=(7, 5))\n",
    "sns.countplot(x=\"sentiment\", data=df, order=[\"Positive\", \"Neutral\", \"Negative\"])\n",
    "plt.title(\"Sentiment Distribution (TextBlob)\")\n",
    "plt.xlabel(\"Sentiment\")\n",
    "plt.ylabel(\"Number of Tweets\")\n",
    "plt.show()\n",
    "\n",
    "# Average polarity\n",
    "avg_sentiment = df[\"polarity\"].mean()\n",
    "print(f\"Average Sentiment Polarity: {avg_sentiment:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01bd2143",
   "metadata": {},
   "source": [
    "#### Using VADER (Better for social media text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faabeab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Apply VADER to each tweet\n",
    "df[\"vader_scores\"] = df[\"cleaned_joined\"].apply(\n",
    "    lambda x: analyzer.polarity_scores(str(x))[\"compound\"]\n",
    ")\n",
    "\n",
    "# Convert to categorical sentiment\n",
    "df[\"vader_sentiment\"] = df[\"vader_scores\"].apply(\n",
    "    lambda s: \"Positive\" if s > 0.05 else (\"Negative\" if s < -0.05 else \"Neutral\")\n",
    ")\n",
    "\n",
    "# Visualization\n",
    "plt.figure(figsize=(7, 5))\n",
    "sns.countplot(\n",
    "    x=\"vader_sentiment\",\n",
    "    data=df,\n",
    "    order=[\"Positive\", \"Neutral\", \"Negative\"],\n",
    "    palette=\"pastel\",\n",
    ")\n",
    "plt.title(\"Sentiment Distribution (VADER)\")\n",
    "plt.xlabel(\"Sentiment\")\n",
    "plt.ylabel(\"Number of Tweets\")\n",
    "plt.show()\n",
    "\n",
    "# Average sentiment score\n",
    "avg_vader = df[\"vader_scores\"].mean()\n",
    "print(f\"Average VADER Sentiment Score: {avg_vader:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "117b1914",
   "metadata": {},
   "source": [
    "### Interpretation of Results\n",
    "\n",
    "From the TextBlob and VADER analyses, we can observe that:\n",
    "\n",
    "- The majority of tweets around **#ElectricVehicles** are positive, showing enthusiasm about EV technology and sustainability.\n",
    "- Neutral tweets mostly share factual updates (e.g., government policy or product announcements).\n",
    "- Negative tweets often discuss high costs, range anxiety, or charging challenges.\n",
    "\n",
    "The VADER model produced slightly more neutral classifications compared to TextBlob, which tends to exaggerate positive sentiment. The optional Transformer model provides more context-aware scoring and aligns more closely with nuanced opinions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa9c0999",
   "metadata": {},
   "source": [
    "## 4. Word Cloud Visualization\n",
    "\n",
    "To complement the sentiment analysis, we will visualize the most frequent words used in the tweets through a **word cloud**.  \n",
    "A word cloud shows words sized proportionally to their frequency — larger words appear more often in the dataset.\n",
    "\n",
    "This helps identify common themes or discussion points around our topic (**#ElectricVehicles**).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3b2e6ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud, STOPWORDS\n",
    "\n",
    "# Combine all cleaned text\n",
    "text_all = \" \".join(df[\"cleaned_joined\"].astype(str))\n",
    "\n",
    "# Add default and custom stopwords to filter out uninformative words\n",
    "stopwords = set(STOPWORDS)\n",
    "custom_stopwords = {\"https\", \"co\", \"rt\", \"amp\"}  # common Twitter noise\n",
    "stopwords.update(custom_stopwords)\n",
    "\n",
    "# Generate the word cloud\n",
    "wordcloud = WordCloud(\n",
    "    width=1000,\n",
    "    height=600,\n",
    "    background_color=\"white\",\n",
    "    stopwords=stopwords,\n",
    "    collocations=False,\n",
    "    max_words=150,\n",
    ").generate(text_all)\n",
    "\n",
    "# Display it\n",
    "plt.figure(figsize=(12, 7))\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Most Frequent Words in #ElectricVehicles Tweets\", fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9309fd7",
   "metadata": {},
   "source": [
    "### Interpretation of Themes\n",
    "\n",
    "- Words such as **“tesla”, “battery”, “charging”, “price”, “sustainability”** may appear most frequently, reflecting public focus on technology, cost, and environmental impact.\n",
    "- If policy-related terms appear (e.g., “subsidy”, “government”, “incentive”), they indicate discussion around regulation and infrastructure.\n",
    "- The presence of **positive** words (e.g., “love”, “future”, “innovation”) or **negative** ones (e.g., “expensive”, “problem”, “delay”) complements the sentiment analysis findings.\n",
    "\n",
    "### Summary of Step 4\n",
    "\n",
    "The word cloud provides a quick visual summary of the conversation landscape and helps identify keywords driving positive or negative sentiment.  \n",
    "Together with Step 3’s sentiment plots, it gives a comprehensive overview of public mood toward **#ElectricVehicles**.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Applications-of-AI-ItjPKJgw",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
