{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "718a467a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import snscrape.modules.twitter as sntwitter #only works with python 3.11 or lower\n",
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import spacy\n",
    "import emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fb29a394",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error retrieving https://twitter.com/i/api/graphql/7jT5GT59P8IFjgxwqnEdQw/SearchTimeline?variables=%7B%22rawQuery%22%3A%22%23ElectricVehicles%20since%3A2024-11-01%20until%3A2024-11-04%20lang%3Aen%22%2C%22count%22%3A20%2C%22product%22%3A%22Latest%22%2C%22withDownvotePerspective%22%3Afalse%2C%22withReactionsMetadata%22%3Afalse%2C%22withReactionsPerspective%22%3Afalse%7D&features=%7B%22rweb_lists_timeline_redesign_enabled%22%3Afalse%2C%22blue_business_profile_image_shape_enabled%22%3Afalse%2C%22responsive_web_graphql_exclude_directive_enabled%22%3Atrue%2C%22verified_phone_label_enabled%22%3Afalse%2C%22creator_subscriptions_tweet_preview_api_enabled%22%3Afalse%2C%22responsive_web_graphql_timeline_navigation_enabled%22%3Atrue%2C%22responsive_web_graphql_skip_user_profile_image_extensions_enabled%22%3Afalse%2C%22tweetypie_unmention_optimization_enabled%22%3Atrue%2C%22vibe_api_enabled%22%3Atrue%2C%22responsive_web_edit_tweet_api_enabled%22%3Atrue%2C%22graphql_is_translatable_rweb_tweet_is_translatable_enabled%22%3Atrue%2C%22view_counts_everywhere_api_enabled%22%3Atrue%2C%22longform_notetweets_consumption_enabled%22%3Atrue%2C%22tweet_awards_web_tipping_enabled%22%3Afalse%2C%22freedom_of_speech_not_reach_fetch_enabled%22%3Afalse%2C%22standardized_nudges_misinfo%22%3Atrue%2C%22tweet_with_visibility_results_prefer_gql_limited_actions_policy_enabled%22%3Afalse%2C%22interactive_text_enabled%22%3Atrue%2C%22responsive_web_text_conversations_enabled%22%3Afalse%2C%22longform_notetweets_rich_text_read_enabled%22%3Afalse%2C%22longform_notetweets_inline_media_enabled%22%3Afalse%2C%22responsive_web_enhance_cards_enabled%22%3Afalse%2C%22responsive_web_twitter_blue_verified_badge_is_enabled%22%3Atrue%7D: blocked (404)\n",
      "4 requests to https://twitter.com/i/api/graphql/7jT5GT59P8IFjgxwqnEdQw/SearchTimeline?variables=%7B%22rawQuery%22%3A%22%23ElectricVehicles%20since%3A2024-11-01%20until%3A2024-11-04%20lang%3Aen%22%2C%22count%22%3A20%2C%22product%22%3A%22Latest%22%2C%22withDownvotePerspective%22%3Afalse%2C%22withReactionsMetadata%22%3Afalse%2C%22withReactionsPerspective%22%3Afalse%7D&features=%7B%22rweb_lists_timeline_redesign_enabled%22%3Afalse%2C%22blue_business_profile_image_shape_enabled%22%3Afalse%2C%22responsive_web_graphql_exclude_directive_enabled%22%3Atrue%2C%22verified_phone_label_enabled%22%3Afalse%2C%22creator_subscriptions_tweet_preview_api_enabled%22%3Afalse%2C%22responsive_web_graphql_timeline_navigation_enabled%22%3Atrue%2C%22responsive_web_graphql_skip_user_profile_image_extensions_enabled%22%3Afalse%2C%22tweetypie_unmention_optimization_enabled%22%3Atrue%2C%22vibe_api_enabled%22%3Atrue%2C%22responsive_web_edit_tweet_api_enabled%22%3Atrue%2C%22graphql_is_translatable_rweb_tweet_is_translatable_enabled%22%3Atrue%2C%22view_counts_everywhere_api_enabled%22%3Atrue%2C%22longform_notetweets_consumption_enabled%22%3Atrue%2C%22tweet_awards_web_tipping_enabled%22%3Afalse%2C%22freedom_of_speech_not_reach_fetch_enabled%22%3Afalse%2C%22standardized_nudges_misinfo%22%3Atrue%2C%22tweet_with_visibility_results_prefer_gql_limited_actions_policy_enabled%22%3Afalse%2C%22interactive_text_enabled%22%3Atrue%2C%22responsive_web_text_conversations_enabled%22%3Afalse%2C%22longform_notetweets_rich_text_read_enabled%22%3Afalse%2C%22longform_notetweets_inline_media_enabled%22%3Afalse%2C%22responsive_web_enhance_cards_enabled%22%3Afalse%2C%22responsive_web_twitter_blue_verified_badge_is_enabled%22%3Atrue%7D failed, giving up.\n",
      "Errors: blocked (404), blocked (404), blocked (404), blocked (404)\n"
     ]
    },
    {
     "ename": "ScraperException",
     "evalue": "4 requests to https://twitter.com/i/api/graphql/7jT5GT59P8IFjgxwqnEdQw/SearchTimeline?variables=%7B%22rawQuery%22%3A%22%23ElectricVehicles%20since%3A2024-11-01%20until%3A2024-11-04%20lang%3Aen%22%2C%22count%22%3A20%2C%22product%22%3A%22Latest%22%2C%22withDownvotePerspective%22%3Afalse%2C%22withReactionsMetadata%22%3Afalse%2C%22withReactionsPerspective%22%3Afalse%7D&features=%7B%22rweb_lists_timeline_redesign_enabled%22%3Afalse%2C%22blue_business_profile_image_shape_enabled%22%3Afalse%2C%22responsive_web_graphql_exclude_directive_enabled%22%3Atrue%2C%22verified_phone_label_enabled%22%3Afalse%2C%22creator_subscriptions_tweet_preview_api_enabled%22%3Afalse%2C%22responsive_web_graphql_timeline_navigation_enabled%22%3Atrue%2C%22responsive_web_graphql_skip_user_profile_image_extensions_enabled%22%3Afalse%2C%22tweetypie_unmention_optimization_enabled%22%3Atrue%2C%22vibe_api_enabled%22%3Atrue%2C%22responsive_web_edit_tweet_api_enabled%22%3Atrue%2C%22graphql_is_translatable_rweb_tweet_is_translatable_enabled%22%3Atrue%2C%22view_counts_everywhere_api_enabled%22%3Atrue%2C%22longform_notetweets_consumption_enabled%22%3Atrue%2C%22tweet_awards_web_tipping_enabled%22%3Afalse%2C%22freedom_of_speech_not_reach_fetch_enabled%22%3Afalse%2C%22standardized_nudges_misinfo%22%3Atrue%2C%22tweet_with_visibility_results_prefer_gql_limited_actions_policy_enabled%22%3Afalse%2C%22interactive_text_enabled%22%3Atrue%2C%22responsive_web_text_conversations_enabled%22%3Afalse%2C%22longform_notetweets_rich_text_read_enabled%22%3Afalse%2C%22longform_notetweets_inline_media_enabled%22%3Afalse%2C%22responsive_web_enhance_cards_enabled%22%3Afalse%2C%22responsive_web_twitter_blue_verified_badge_is_enabled%22%3Atrue%7D failed, giving up.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mScraperException\u001b[39m                          Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      4\u001b[39m tweets = []\n\u001b[32m      5\u001b[39m limit = \u001b[32m10\u001b[39m \u001b[38;5;66;03m# number of tweets to collect, limit per month is 100 posts\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m \u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtweet\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msntwitter\u001b[49m\u001b[43m.\u001b[49m\u001b[43mTwitterSearchScraper\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_items\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[43m>\u001b[49m\u001b[43m \u001b[49m\u001b[43mlimit\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mbreak\u001b[39;49;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.virtualenvs/Applications-of-AI-ItjPKJgw/lib/python3.11/site-packages/snscrape/modules/twitter.py:1763\u001b[39m, in \u001b[36mTwitterSearchScraper.get_items\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1760\u001b[39m params = {\u001b[33m'\u001b[39m\u001b[33mvariables\u001b[39m\u001b[33m'\u001b[39m: variables, \u001b[33m'\u001b[39m\u001b[33mfeatures\u001b[39m\u001b[33m'\u001b[39m: features}\n\u001b[32m   1761\u001b[39m paginationParams = {\u001b[33m'\u001b[39m\u001b[33mvariables\u001b[39m\u001b[33m'\u001b[39m: paginationVariables, \u001b[33m'\u001b[39m\u001b[33mfeatures\u001b[39m\u001b[33m'\u001b[39m: features}\n\u001b[32m-> \u001b[39m\u001b[32m1763\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_iter_api_data\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mhttps://twitter.com/i/api/graphql/7jT5GT59P8IFjgxwqnEdQw/SearchTimeline\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_TwitterAPIType\u001b[49m\u001b[43m.\u001b[49m\u001b[43mGRAPHQL\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpaginationParams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcursor\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_cursor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minstructionsPath\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mdata\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43msearch_by_raw_query\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43msearch_timeline\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mtimeline\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43minstructions\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   1764\u001b[39m \u001b[43m\t\u001b[49m\u001b[38;5;28;43;01myield from\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_graphql_timeline_instructions_to_tweets\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mdata\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43msearch_by_raw_query\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43msearch_timeline\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mtimeline\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43minstructions\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.virtualenvs/Applications-of-AI-ItjPKJgw/lib/python3.11/site-packages/snscrape/modules/twitter.py:915\u001b[39m, in \u001b[36m_TwitterAPIScraper._iter_api_data\u001b[39m\u001b[34m(self, endpoint, apiType, params, paginationParams, cursor, direction, instructionsPath)\u001b[39m\n\u001b[32m    913\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m    914\u001b[39m \t_logger.info(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mRetrieving scroll page \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcursor\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m915\u001b[39m \tobj = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_api_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mendpoint\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mapiType\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreqParams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minstructionsPath\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43minstructionsPath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    916\u001b[39m \t\u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[32m    918\u001b[39m \t\u001b[38;5;66;03m# No data format test, just a hard and loud crash if anything's wrong :-)\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.virtualenvs/Applications-of-AI-ItjPKJgw/lib/python3.11/site-packages/snscrape/modules/twitter.py:886\u001b[39m, in \u001b[36m_TwitterAPIScraper._get_api_data\u001b[39m\u001b[34m(self, endpoint, apiType, params, instructionsPath)\u001b[39m\n\u001b[32m    884\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m apiType \u001b[38;5;129;01mis\u001b[39;00m _TwitterAPIType.GRAPHQL:\n\u001b[32m    885\u001b[39m \tparams = urllib.parse.urlencode({k: json.dumps(v, separators = (\u001b[33m'\u001b[39m\u001b[33m,\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33m:\u001b[39m\u001b[33m'\u001b[39m)) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m params.items()}, quote_via = urllib.parse.quote)\n\u001b[32m--> \u001b[39m\u001b[32m886\u001b[39m r = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get\u001b[49m\u001b[43m(\u001b[49m\u001b[43mendpoint\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_apiHeaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponseOkCallback\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunctools\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpartial\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_check_api_response\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mapiType\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mapiType\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minstructionsPath\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43minstructionsPath\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    887\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m r._snscrapeObj\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.virtualenvs/Applications-of-AI-ItjPKJgw/lib/python3.11/site-packages/snscrape/base.py:275\u001b[39m, in \u001b[36mScraper._get\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    274\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_get\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args, **kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m275\u001b[39m \t\u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mGET\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.virtualenvs/Applications-of-AI-ItjPKJgw/lib/python3.11/site-packages/snscrape/base.py:271\u001b[39m, in \u001b[36mScraper._request\u001b[39m\u001b[34m(self, method, url, params, data, headers, timeout, responseOkCallback, allowRedirects, proxies)\u001b[39m\n\u001b[32m    269\u001b[39m \t_logger.fatal(msg)\n\u001b[32m    270\u001b[39m \t_logger.fatal(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mErrors: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[33m\"\u001b[39m\u001b[33m, \u001b[39m\u001b[33m\"\u001b[39m.join(errors)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m271\u001b[39m \t\u001b[38;5;28;01mraise\u001b[39;00m ScraperException(msg)\n\u001b[32m    272\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33m'\u001b[39m\u001b[33mReached unreachable code\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[31mScraperException\u001b[39m: 4 requests to https://twitter.com/i/api/graphql/7jT5GT59P8IFjgxwqnEdQw/SearchTimeline?variables=%7B%22rawQuery%22%3A%22%23ElectricVehicles%20since%3A2024-11-01%20until%3A2024-11-04%20lang%3Aen%22%2C%22count%22%3A20%2C%22product%22%3A%22Latest%22%2C%22withDownvotePerspective%22%3Afalse%2C%22withReactionsMetadata%22%3Afalse%2C%22withReactionsPerspective%22%3Afalse%7D&features=%7B%22rweb_lists_timeline_redesign_enabled%22%3Afalse%2C%22blue_business_profile_image_shape_enabled%22%3Afalse%2C%22responsive_web_graphql_exclude_directive_enabled%22%3Atrue%2C%22verified_phone_label_enabled%22%3Afalse%2C%22creator_subscriptions_tweet_preview_api_enabled%22%3Afalse%2C%22responsive_web_graphql_timeline_navigation_enabled%22%3Atrue%2C%22responsive_web_graphql_skip_user_profile_image_extensions_enabled%22%3Afalse%2C%22tweetypie_unmention_optimization_enabled%22%3Atrue%2C%22vibe_api_enabled%22%3Atrue%2C%22responsive_web_edit_tweet_api_enabled%22%3Atrue%2C%22graphql_is_translatable_rweb_tweet_is_translatable_enabled%22%3Atrue%2C%22view_counts_everywhere_api_enabled%22%3Atrue%2C%22longform_notetweets_consumption_enabled%22%3Atrue%2C%22tweet_awards_web_tipping_enabled%22%3Afalse%2C%22freedom_of_speech_not_reach_fetch_enabled%22%3Afalse%2C%22standardized_nudges_misinfo%22%3Atrue%2C%22tweet_with_visibility_results_prefer_gql_limited_actions_policy_enabled%22%3Afalse%2C%22interactive_text_enabled%22%3Atrue%2C%22responsive_web_text_conversations_enabled%22%3Afalse%2C%22longform_notetweets_rich_text_read_enabled%22%3Afalse%2C%22longform_notetweets_inline_media_enabled%22%3Afalse%2C%22responsive_web_enhance_cards_enabled%22%3Afalse%2C%22responsive_web_twitter_blue_verified_badge_is_enabled%22%3Atrue%7D failed, giving up."
     ]
    }
   ],
   "source": [
    "# Define your search topic\n",
    "query = \"#ElectricVehicles since:2024-11-01 until:2024-11-04 lang:en\"\n",
    "\n",
    "tweets = []\n",
    "limit = 10 # number of tweets to collect, limit per month is 100 posts\n",
    "\n",
    "\n",
    "for i, tweet in enumerate(sntwitter.TwitterSearchScraper(query).get_items()):\n",
    "    if i > limit:\n",
    "        break\n",
    "    tweets.append([tweet.date, tweet.user.username, tweet.content])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d5960a90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Date",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "User",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Tweet",
         "rawType": "object",
         "type": "string"
        }
       ],
       "ref": "26d03a31-09d8-4913-a276-bbba01001ea4",
       "rows": [],
       "shape": {
        "columns": 3,
        "rows": 0
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>User</th>\n",
       "      <th>Tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Date, User, Tweet]\n",
       "Index: []"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create DataFrame\n",
    "df = pd.DataFrame(tweets, columns=[\"Date\", \"User\", \"Tweet\"])\n",
    "df.to_csv(\"tweets_ai.csv\", index=False)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6d55b8d",
   "metadata": {},
   "source": [
    "## 2. Data Cleaning and Preparation\n",
    "\n",
    "Clean and prepare the tweet text data for sentiment analysis. \n",
    "The steps include:\n",
    "\n",
    "1. Removing URLs, mentions, hashtags (or removing the ‘#’ but keeping the word), emojis and special characters  \n",
    "2. Converting text to lowercase  \n",
    "3. Tokenizing the text  \n",
    "4. Removing stop-words  \n",
    "5. Lemmatizing the tokens  \n",
    "6. Optionally rebuilding the cleaned text field  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d4a071ae",
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[E050] Can't find model 'en_core_web_sm'. It doesn't seem to be a Python package or a valid path to a data directory.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOSError\u001b[39m                                   Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Load spaCy English model\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m nlp = \u001b[43mspacy\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43men_core_web_sm\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# Define stopwords\u001b[39;00m\n\u001b[32m      5\u001b[39m stop_words = \u001b[38;5;28mset\u001b[39m(stopwords.words(\u001b[33m'\u001b[39m\u001b[33menglish\u001b[39m\u001b[33m'\u001b[39m))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.virtualenvs/Applications-of-AI-ItjPKJgw/lib/python3.11/site-packages/spacy/__init__.py:52\u001b[39m, in \u001b[36mload\u001b[39m\u001b[34m(name, vocab, disable, enable, exclude, config)\u001b[39m\n\u001b[32m     28\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mload\u001b[39m(\n\u001b[32m     29\u001b[39m     name: Union[\u001b[38;5;28mstr\u001b[39m, Path],\n\u001b[32m     30\u001b[39m     *,\n\u001b[32m   (...)\u001b[39m\u001b[32m     35\u001b[39m     config: Union[Dict[\u001b[38;5;28mstr\u001b[39m, Any], Config] = util.SimpleFrozenDict(),\n\u001b[32m     36\u001b[39m ) -> Language:\n\u001b[32m     37\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Load a spaCy model from an installed package or a local path.\u001b[39;00m\n\u001b[32m     38\u001b[39m \n\u001b[32m     39\u001b[39m \u001b[33;03m    name (str): Package name or model path.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     50\u001b[39m \u001b[33;03m    RETURNS (Language): The loaded nlp object.\u001b[39;00m\n\u001b[32m     51\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mutil\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     53\u001b[39m \u001b[43m        \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     54\u001b[39m \u001b[43m        \u001b[49m\u001b[43mvocab\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvocab\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     55\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdisable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdisable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     56\u001b[39m \u001b[43m        \u001b[49m\u001b[43menable\u001b[49m\u001b[43m=\u001b[49m\u001b[43menable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     57\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexclude\u001b[49m\u001b[43m=\u001b[49m\u001b[43mexclude\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     58\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     59\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.virtualenvs/Applications-of-AI-ItjPKJgw/lib/python3.11/site-packages/spacy/util.py:484\u001b[39m, in \u001b[36mload_model\u001b[39m\u001b[34m(name, vocab, disable, enable, exclude, config)\u001b[39m\n\u001b[32m    482\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m OLD_MODEL_SHORTCUTS:\n\u001b[32m    483\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIOError\u001b[39;00m(Errors.E941.format(name=name, full=OLD_MODEL_SHORTCUTS[name]))  \u001b[38;5;66;03m# type: ignore[index]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m484\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIOError\u001b[39;00m(Errors.E050.format(name=name))\n",
      "\u001b[31mOSError\u001b[39m: [E050] Can't find model 'en_core_web_sm'. It doesn't seem to be a Python package or a valid path to a data directory."
     ]
    }
   ],
   "source": [
    "# Load spaCy English model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Define stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# A function to remove URLs, mentions, hashtags (#), emojis, special chars\n",
    "def clean_tweet_text(text):\n",
    "    # Remove URLs\n",
    "    text = re.sub(r\"http\\S+|www.\\S+\", \"\", text)\n",
    "    # Remove mentions (@username)\n",
    "    text = re.sub(r\"@\\w+\", \"\", text)\n",
    "    # Remove hashtag symbol (keep the word) – e.g. \"#ElectricVehicles\" → \"ElectricVehicles\"\n",
    "    text = re.sub(r\"#\", \"\", text)\n",
    "    # Demojize (optional) or simply remove emojis\n",
    "    text = emoji.replace_emoji(text, replace=\"\")\n",
    "    # Remove punctuation and special characters\n",
    "    text = re.sub(f\"[{re.escape(string.punctuation)}]\", \"\", text)\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    return text\n",
    "\n",
    "# A function to tokenize, remove stopwords, and lemmatize\n",
    "def tokenize_lemmatize(text):\n",
    "    # Tokenize\n",
    "    tokens = word_tokenize(text)\n",
    "    # Remove stopwords and tokens of length <= 1\n",
    "    tokens = [tok for tok in tokens if tok not in stop_words and len(tok) > 1]\n",
    "    # Lemmatize using spaCy\n",
    "    doc = nlp(\" \".join(tokens))\n",
    "    lemmas = [token.lemma_ for token in doc]\n",
    "    return lemmas\n",
    "\n",
    "# Example usage: load the data\n",
    "df = pd.read_csv(\"tweets_evs.csv\")   # make sure your filename matches\n",
    "# Apply cleaning\n",
    "df['clean_text'] = df['Tweet'].apply(clean_tweet_text)\n",
    "df['tokens'] = df['clean_text'].apply(tokenize_lemmatize)\n",
    "# Optionally join tokens back to a cleaned string\n",
    "df['cleaned_joined'] = df['tokens'].apply(lambda x: \" \".join(x))\n",
    "\n",
    "# Show the first few cleaned rows\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e58609d4",
   "metadata": {},
   "source": [
    "### Explanation of cleaning steps:\n",
    "- We start by removing URLs and mentions because they don’t contribute to sentiment in most cases and may bias the text.  \n",
    "- We remove the “#” symbol but keep the keyword after it so that hashtags still contribute their meaning (for example, ‘ElectricVehicles’ becomes a regular word).  \n",
    "- Emojis can carry sentiment but for simplicity we remove them here (you could instead convert emojis to text meaning using `emoji.demojize`).  \n",
    "- Punctuation and special characters are removed to reduce noise.  \n",
    "- Lowercasing ensures that words like “EVs”, “evs”, and “Evs” are treated the same.  \n",
    "- Tokenization breaks the text into individual tokens/words.  \n",
    "- Stop-words removal strips out very common words (“the”, “is”, “and”) that don’t carry sentiment by themselves.  \n",
    "- Lemmatization reduces words to their base form (“running” → “run”, “better” → “good”), which helps with grouping similar concepts.  \n",
    "- Finally, we rebuild a cleaned version of the text for downstream analysis (sentiment scoring, word-cloud, etc).  \n",
    "\n",
    "```python\n",
    "# Save cleaned data (optional)\n",
    "df.to_csv(\"tweets_evs_cleaned.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa003e1a",
   "metadata": {},
   "source": [
    "## 3. Sentiment Analysis\n",
    "\n",
    "In this section, we will analyze the sentiment of our cleaned tweets using three different approaches:\n",
    "1. **TextBlob** – simple polarity-based sentiment scoring  \n",
    "2. **VADER (Valence Aware Dictionary for sEntiment Reasoning)** – optimized for social media text  \n",
    "3. *(Optional)* **Transformer model (HuggingFace)** – a pretrained deep learning model for more accurate sentiment classification  \n",
    "\n",
    "For each tweet, we will compute its sentiment polarity and then visualize the distribution of sentiments (positive, neutral, negative).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "557cc161",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from textblob import TextBlob\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Load cleaned data\n",
    "df = pd.read_csv(\"tweets_evs_cleaned.csv\")\n",
    "\n",
    "# Compute polarity: range is [-1.0, 1.0]\n",
    "df['polarity'] = df['cleaned_joined'].apply(lambda x: TextBlob(str(x)).sentiment.polarity)\n",
    "\n",
    "# Define sentiment categories\n",
    "def get_sentiment_label(p):\n",
    "    if p > 0.05:\n",
    "        return 'Positive'\n",
    "    elif p < -0.05:\n",
    "        return 'Negative'\n",
    "    else:\n",
    "        return 'Neutral'\n",
    "\n",
    "df['sentiment'] = df['polarity'].apply(get_sentiment_label)\n",
    "\n",
    "# Display sample results\n",
    "df[['cleaned_joined', 'polarity', 'sentiment']].head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f07469af",
   "metadata": {},
   "source": [
    "### Visualizing sentiment distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45ba8c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentiment distribution\n",
    "sns.set(style=\"whitegrid\")\n",
    "plt.figure(figsize=(7,5))\n",
    "sns.countplot(x='sentiment', data=df, order=['Positive', 'Neutral', 'Negative'])\n",
    "plt.title(\"Sentiment Distribution (TextBlob)\")\n",
    "plt.xlabel(\"Sentiment\")\n",
    "plt.ylabel(\"Number of Tweets\")\n",
    "plt.show()\n",
    "\n",
    "# Average polarity\n",
    "avg_sentiment = df['polarity'].mean()\n",
    "print(f\"Average Sentiment Polarity: {avg_sentiment:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01bd2143",
   "metadata": {},
   "source": [
    "#### Using VADER (Better for social media text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faabeab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Apply VADER to each tweet\n",
    "df['vader_scores'] = df['cleaned_joined'].apply(lambda x: analyzer.polarity_scores(str(x))['compound'])\n",
    "\n",
    "# Convert to categorical sentiment\n",
    "df['vader_sentiment'] = df['vader_scores'].apply(lambda s: 'Positive' if s > 0.05 else ('Negative' if s < -0.05 else 'Neutral'))\n",
    "\n",
    "# Visualization\n",
    "plt.figure(figsize=(7,5))\n",
    "sns.countplot(x='vader_sentiment', data=df, order=['Positive', 'Neutral', 'Negative'], palette='pastel')\n",
    "plt.title(\"Sentiment Distribution (VADER)\")\n",
    "plt.xlabel(\"Sentiment\")\n",
    "plt.ylabel(\"Number of Tweets\")\n",
    "plt.show()\n",
    "\n",
    "# Average sentiment score\n",
    "avg_vader = df['vader_scores'].mean()\n",
    "print(f\"Average VADER Sentiment Score: {avg_vader:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "117b1914",
   "metadata": {},
   "source": [
    "### Interpretation of Results\n",
    "\n",
    "From the TextBlob and VADER analyses, we can observe that:\n",
    "- The majority of tweets around **#ElectricVehicles** are positive, showing enthusiasm about EV technology and sustainability.\n",
    "- Neutral tweets mostly share factual updates (e.g., government policy or product announcements).\n",
    "- Negative tweets often discuss high costs, range anxiety, or charging challenges.\n",
    "\n",
    "The VADER model produced slightly more neutral classifications compared to TextBlob, which tends to exaggerate positive sentiment. The optional Transformer model provides more context-aware scoring and aligns more closely with nuanced opinions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa9c0999",
   "metadata": {},
   "source": [
    "## 4. Word Cloud Visualization\n",
    "\n",
    "To complement the sentiment analysis, we will visualize the most frequent words used in the tweets through a **word cloud**.  \n",
    "A word cloud shows words sized proportionally to their frequency — larger words appear more often in the dataset.\n",
    "\n",
    "This helps identify common themes or discussion points around our topic (**#ElectricVehicles**).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3b2e6ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud, STOPWORDS\n",
    "\n",
    "# Combine all cleaned text\n",
    "text_all = \" \".join(df['cleaned_joined'].astype(str))\n",
    "\n",
    "# Add default and custom stopwords to filter out uninformative words\n",
    "stopwords = set(STOPWORDS)\n",
    "custom_stopwords = {\"https\", \"co\", \"rt\", \"amp\"}  # common Twitter noise\n",
    "stopwords.update(custom_stopwords)\n",
    "\n",
    "# Generate the word cloud\n",
    "wordcloud = WordCloud(width=1000,\n",
    "                      height=600,\n",
    "                      background_color='white',\n",
    "                      stopwords=stopwords,\n",
    "                      collocations=False,\n",
    "                      max_words=150).generate(text_all)\n",
    "\n",
    "# Display it\n",
    "plt.figure(figsize=(12, 7))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.title(\"Most Frequent Words in #ElectricVehicles Tweets\", fontsize=14)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9309fd7",
   "metadata": {},
   "source": [
    "### Interpretation of Themes\n",
    "- Words such as **“tesla”, “battery”, “charging”, “price”, “sustainability”** may appear most frequently, reflecting public focus on technology, cost, and environmental impact.  \n",
    "- If policy-related terms appear (e.g., “subsidy”, “government”, “incentive”), they indicate discussion around regulation and infrastructure.  \n",
    "- The presence of **positive** words (e.g., “love”, “future”, “innovation”) or **negative** ones (e.g., “expensive”, “problem”, “delay”) complements the sentiment analysis findings.\n",
    "\n",
    "### Summary of Step 4\n",
    "The word cloud provides a quick visual summary of the conversation landscape and helps identify keywords driving positive or negative sentiment.  \n",
    "Together with Step 3’s sentiment plots, it gives a comprehensive overview of public mood toward **#ElectricVehicles**.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Applications-of-AI-ItjPKJgw",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
